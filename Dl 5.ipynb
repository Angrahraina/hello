{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5pKXLDiTegT"
   },
   "source": [
    "5.Cbow stages\n",
    "obj-to learn and understand cbow\n",
    "CBOW, which stands for \"Continuous Bag of Words,\" is a type of word embedding model used in natural language processing and deep learning. It is designed to learn distributed representations (word vectors) of words in a large text corpus. CBOW is one of two popular architectures for word embeddings, with the other being Skip-gram.\n",
    "\n",
    "The primary goal of CBOW is to predict a target word based on the context words that surround it. It operates as follows:\n",
    "\n",
    "1. **Data Preparation:** To train a CBOW model, a large text corpus is required. The corpus is tokenized into sentences, and each sentence is further divided into words or subword units, such as tokens or subword embeddings.\n",
    "\n",
    "2. **Context Window:** CBOW defines a context window around each target word. The context window size specifies how many words before and after the target word are considered as context words. For example, if the context window size is 2, and the target word is \"cat\" in the sentence \"The quick brown cat jumps,\" the context words would be \"quick,\" \"brown,\" \"jumps.\"\n",
    "\n",
    "3. **Word Embeddings:** Each word in the vocabulary is associated with a unique word embedding vector. These word embeddings are learned during the training process, and their dimensions are typically set as hyperparameters.\n",
    "\n",
    "4. **Model Architecture:** The CBOW model architecture consists of an input layer, a hidden layer (word embeddings), and an output layer. The input layer encodes the context words, and the output layer aims to predict the target word. The hidden layer is the sum of the word embeddings for the context words.\n",
    "\n",
    "5. **Training Objective:** The CBOW model is trained to minimize the difference between its predicted output and the actual target word. This is typically done using a softmax activation function and a categorical cross-entropy loss function. The goal is to make the model's predictions for the target word as accurate as possible given the context words.\n",
    "\n",
    "6. **Word Vector Learning:** During training, the word embeddings are updated using backpropagation and stochastic gradient descent (SGD) to minimize the loss function. As training progresses, the word embeddings become more representative of the words' meanings based on their co-occurrence patterns in the corpus.\n",
    "\n",
    "The learned word embeddings capture the semantic relationships between words. Words with similar meanings or similar usage tend to have similar word vectors, making them useful for various natural language processing tasks, such as sentiment analysis, text classification, machine translation, and more. These word embeddings are pre-trained on large corpora and can be used as features in downstream NLP models. Popular pre-trained word embeddings, such as Word2Vec, GloVe, and FastText, are often based on CBOW or Skip-gram models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "p1Dl_JiWs6TF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "from keras import utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "snW8tGaetMWl"
   },
   "outputs": [],
   "source": [
    "#pip install keras.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BKG4Q5W4t2bf"
   },
   "outputs": [],
   "source": [
    "data=open('corona.txt','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fUsCUZokv2Jb"
   },
   "outputs": [],
   "source": [
    "corona_data = [text for text in data if text.count(' ') >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IGYCGqB1v6Jt"
   },
   "outputs": [],
   "source": [
    "vectorize = Tokenizer()\n",
    "vectorize.fit_on_texts(corona_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hGHQc9Byv9pA"
   },
   "outputs": [],
   "source": [
    "corona_data = vectorize.texts_to_sequences(corona_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "912pzy7AwBsj"
   },
   "outputs": [],
   "source": [
    "total_vocab = sum(len(s) for s in corona_data)\n",
    "word_count = len(vectorize.word_index) + 1\n",
    "window_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vwt5pZH8wKk8"
   },
   "outputs": [],
   "source": [
    "def cbow_model(data, window_size, total_vocab):\n",
    "    total_length = window_size*2\n",
    "    for text in data:\n",
    "        text_len = len(text)\n",
    "        for idx, word in enumerate(text):\n",
    "            context_word = []\n",
    "            target   = []\n",
    "            begin = idx - window_size\n",
    "            end = idx + window_size + 1\n",
    "            context_word.append([text[i] for i in range(begin, end) if 0 <= i < text_len and i != idx])\n",
    "            target.append(word)\n",
    "            contextual = sequence.pad_sequences(context_word, total_length=total_length)\n",
    "            final_target = utils.to_categorical(target, total_vocab)\n",
    "            yield(contextual, final_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iq3rhDGZwoyi",
    "outputId": "e0cfe599-7623-4587-cd92-7fcdd9b0d726"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 0\n",
      "2 0\n",
      "3 0\n",
      "4 0\n",
      "5 0\n",
      "6 0\n",
      "7 0\n",
      "8 0\n",
      "9 0\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=total_vocab, output_dim=100, input_length=window_size*2))\n",
    "model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(100,)))\n",
    "model.add(Dense(total_vocab, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "for i in range(10):\n",
    "    cost = 0\n",
    "    for x, y in cbow_model(data, window_size, total_vocab):\n",
    "        cost += model.train_on_batch(contextual, final_target)\n",
    "    print(i, cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZSXG7GZawuaB",
    "outputId": "e2ae50a8-c1ff-49a3-b072-c3203abc2113"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimensions=100\n",
    "vect_file = open('vectors.txt','w')\n",
    "vect_file.write('{} {}\\n'.format(101,dimensions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_dTl7J_Qw_dR"
   },
   "outputs": [],
   "source": [
    "weights = model.get_weights()[0]\n",
    "for text, i in vectorize.word_index.items():\n",
    "    final_vec = ' '.join(map(str, list(weights[i, :])))\n",
    "    vect_file.write('{} {}\\n'.format(text, final_vec))\n",
    "vect_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lFBIb9xSxEyX",
    "outputId": "f5f8ecdb-f104-4820-c560-51930fee1f75"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('are', 0.21292082965373993),\n",
       " ('both', 0.15492649376392365),\n",
       " ('shorter', 0.1516490876674652),\n",
       " ('to', 0.14281314611434937),\n",
       " ('context', 0.13967569172382355),\n",
       " ('understood', 0.13876615464687347),\n",
       " ('pre', 0.13234691321849823),\n",
       " ('does', 0.1282172054052353),\n",
       " ('estimated', 0.11985184252262115),\n",
       " ('there', 0.11273766309022903)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_output = gensim.models.KeyedVectors.load_word2vec_format('vectors.txt', binary=False)\n",
    "cbow_output.most_similar(positive=['virus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
